{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6d62d1",
   "metadata": {},
   "source": [
    "# BERT PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b979805",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !/usr/bin/python\n",
    "# Copyright 2022 VMware, Inc.\n",
    "# SPDX-License-Identifier: Apache-2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e607a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for setting seed \n",
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# required to launch training with accelerate in a jupyter notebook\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "# import the config, training function from the BERT Pretraining library\n",
    "from bert_pretraining.bert_pretraining import run_pretraining, Pretraining_Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c16efd",
   "metadata": {},
   "source": [
    "### Fix the seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8cd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed value for reproducability\n",
    "\n",
    "SEED_VALUE = 0\n",
    "random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3027bfe4",
   "metadata": {},
   "source": [
    "### Customize the Config parameters\n",
    "\n",
    "default parameters:\n",
    "\n",
    "\tmodel_name:\tCUSTOM_BERT\n",
    "\tis_base:\tTrue\n",
    "\tmax_seq_length:\t128\n",
    "\tmax_predictions_per_seq:\t20\n",
    "\tnum_train_steps:\t10000\n",
    "\tnum_warmup_steps:\t10\n",
    "\tlearning_rate:\t1e-05\n",
    "\ttrain_batch_size:\t258\n",
    "\tsave_intermediate_checkpoints:\tTrue\n",
    "\tsave_intermediate_checkpoints_steps:\t25000\n",
    "\teval_batch_size:\t258\n",
    "\tmax_eval_steps:\t10000\n",
    "\teval_point:\t10000\n",
    "\tsplit_ratio:\tNone\n",
    "\tinit_checkpoint:\t\n",
    "\tinput_file:\t./input/demo_MSL128.tfrecord\n",
    "\teval_file:\t./input/demo_MSL128.tfrecord\n",
    "\tlog_csv:\t./eval_results.csv\n",
    "\toutput_dir:\t./ckpts\n",
    "\tnum_gpu:\t3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356072cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_config = Pretraining_Config()\n",
    "\n",
    "# Modify your parameters \n",
    "# The parameters listed are for a demo run\n",
    "pretraining_config.model_name = 'DEMOBERT'\n",
    "pretraining_config.train_batch_size = 8\n",
    "pretraining_config.eval_batfch_size = 8\n",
    "pretraining_config.num_train_steps = 500\n",
    "pretraining_config.num_warmup_steps = 8\n",
    "pretraining_config.max_eval_steps = 100\n",
    "pretraining_config.num_gpu = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75986478",
   "metadata": {},
   "source": [
    "### Pretrain your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c1c6b8",
   "metadata": {},
   "source": [
    "Expect the warning :\n",
    "\n",
    "`Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\n",
    "\n",
    "It's telling us to train it on a downstream task before using it for inference but we dont need to worry about it now as we are just pretraining!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a514e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "PRETRAINING_CONFIG PARAMS:\n",
      "\tmodel_name:\tDEMOBERT\n",
      "\tis_base:\tTrue\n",
      "\tmax_seq_length:\t128\n",
      "\tmax_predictions_per_seq:\t20\n",
      "\tnum_train_steps:\t500\n",
      "\tnum_warmup_steps:\t8\n",
      "\tlearning_rate:\t1e-05\n",
      "\ttrain_batch_size:\t8\n",
      "\tsave_intermediate_checkpoints:\tTrue\n",
      "\tsave_intermediate_checkpoints_steps:\t25000\n",
      "\teval_batch_size:\t258\n",
      "\tmax_eval_steps:\t100\n",
      "\teval_point:\t10000\n",
      "\tsplit_ratio:\tNone\n",
      "\tinit_checkpoint:\t\n",
      "\tinput_file:\t./input/demo_MSL128.tfrecord\n",
      "\teval_file:\t./input/demo_MSL128.tfrecord\n",
      "\tlog_csv:\t./eval_results.csv\n",
      "\toutput_dir:\t./ckpts\n",
      "\tnum_gpu:\t2\n",
      "\t\n",
      "DEVICES IN USE:cuda:1\n",
      "\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET PREPARED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_TRAINING BERT MODEL_\n",
      "DEMOBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3b5e6da25f4dfd933900344da11d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:\t:   0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8e9f75fe994caab8d75ab24d862db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3d3f780c464fce872e670df1c2301a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EVALUATING:\t:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8944febffec54344a1a86de0e29d1c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0865b03eec241ba9d69cdf3c99146e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EVALUATING:\t:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45f2b4df1ab4de5a60211cbbf72ab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING THE TRAINING LOG CSV\n",
      "PRETRAINING TIME:\t174.1887822151184\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(run_pretraining, [pretraining_config], num_processes=pretraining_config.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

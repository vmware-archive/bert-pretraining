{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# Copyright 2022 VMware, Inc.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "%%capture\n",
    "#run the bellow lines to install the required packages\n",
    "!pip3 install transformers\n",
    "!pip3 install tfrecord\n",
    "!pip3 install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for setting seed\n",
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# minimum required imports\n",
    "from bert_pretraining import run_pretraining, Pretraining_Config\n",
    "# required to launch training with accelerate in a jupyter notebook\n",
    "from accelerate import notebook_launcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed value for reproducability\n",
    "\n",
    "SEED_VALUE = 0\n",
    "random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize the Config parameters\n",
    "\n",
    "default parameters:\n",
    "\n",
    "\tmodel_name:\tCUSTOM_BERT\n",
    "\tis_base:\tTrue\n",
    "\tmax_seq_length:\t128\n",
    "\tmax_predictions_per_seq:\t20\n",
    "\tnum_train_steps:\t10000\n",
    "\tnum_warmup_steps:\t10\n",
    "\tlearning_rate:\t1e-05\n",
    "\ttrain_batch_size:\t258\n",
    "\tsave_checkpoint:\tTrue\n",
    "\tsave_checkpoint_steps:\t25000\n",
    "\teval_batch_size:\t258\n",
    "\tmax_eval_steps:\t10000\n",
    "\teval_point:\t10000\n",
    "\tsplit_ratio:\tNone\n",
    "\tinit_checkpoint:\t\n",
    "\tinput_file:\t./input/demo_MSL128.tfrecord\n",
    "\teval_file:\t./input/demo_MSL128.tfrecord\n",
    "\tlog_csv:\t./hyperparam_search.csv\n",
    "\toutput_dir:\t./ckpts\n",
    "\tnum_gpu:\t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining_config = Pretraining_Config()\n",
    "\n",
    "# Modify your parameters \n",
    "# The parameters listed are for a demo run\n",
    "pretraining_config.model_name = 'DEMOBERT'\n",
    "pretraining_config.train_batch_size = 32\n",
    "pretraining_config.eval_batfch_size = 32\n",
    "pretraining_config.num_train_steps = 500\n",
    "pretraining_config.num_warmup_steps = 8\n",
    "pretraining_config.max_eval_steps = 100\n",
    "pretraining_config.num_gpu = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrain your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect the warning :\n",
    "\n",
    "`Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.`\n",
    "\n",
    "It's telling us to train it on a downstream task before using it for inference but we dont need to worry about it now as we are just pretraining!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 2 GPUs.\n",
      "PRETRAINING_CONFIG PARAMS:\n",
      "\tmodel_name:\tDEMOBERT\n",
      "\tis_base:\tTrue\n",
      "\tmax_seq_length:\t128\n",
      "\tmax_predictions_per_seq:\t20\n",
      "\tnum_train_steps:\t500\n",
      "\tnum_warmup_steps:\t8\n",
      "\tlearning_rate:\t1e-05\n",
      "\ttrain_batch_size:\t32\n",
      "\tsave_checkpoint:\tTrue\n",
      "\tsave_checkpoint_steps:\t25000\n",
      "\teval_batch_size:\t258\n",
      "\tmax_eval_steps:\t100\n",
      "\teval_point:\t10000\n",
      "\tsplit_ratio:\tNone\n",
      "\tinit_checkpoint:\t\n",
      "\tinput_file:\t./input/demo_MSL128.tfrecord\n",
      "\teval_file:\t./input/demo_MSL128.tfrecord\n",
      "\tlog_csv:\t./hyperparam_search.csv\n",
      "\toutput_dir:\t./ckpts\n",
      "\tnum_gpu:\t2\n",
      "\t\n",
      "DEVICES IN USE:\n",
      "cuda:0\n",
      "cuda:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET PREPARED\n",
      "_TRAINING BERT MODEL_\n",
      "DEMOBERT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a54620e7d3f435eabb1acbf6f96cf05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAINING:\t:   0%|          | 0/508 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67ab6f58548475788f1a7f4182068bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbe2b63014c49c8bbd0bb612b546835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EVALUATING:\t:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e579b52d25414e8f55d4064340a2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1f37921d7a4444afcad5841f311c12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EVALUATING:\t:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05ef34a03644c8a8c506513255dbb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING THE TRAINING LOG CSV\n",
      "TIME:\t179.39882946014404\n"
     ]
    }
   ],
   "source": [
    "notebook_launcher(run_pretraining, [pretraining_config], num_processes=pretraining_config.num_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
